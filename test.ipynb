{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV9NdqOb69Ql"
      },
      "source": [
        "# SetUp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l_FkFMB_EhyM"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import numpy as np\n",
        "from sklearn.utils import resample\n",
        "import zepid\n",
        "from zepid import load_sample_data\n",
        "from zepid.causal.ipw import IPTW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COoqPtol69Qo",
        "outputId": "9551c6bb-392c-4412-ff8c-d672d8096f4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "##################################################\n",
            "## Trial Sequence Object\n",
            "## Estimand: ITT\n",
            "\n",
            "## Data:\n",
            " - N: 725 observations from 89 patients\n",
            "\n",
            "Data Preview:\n",
            "   id  period  treatment  x1        x2  x3        x4  age     age_s  outcome  censored  eligible\n",
            "0   1       0          1   1  1.146148   0  0.734203   36  0.083333        0         0         1\n",
            "1   1       1          1   1  0.002200   0  0.734203   37  0.166667        0         0         0\n",
            "---\n",
            "     id  period  treatment  x1        x2  x3        x4  age     age_s  outcome  censored  eligible\n",
            "723  99       6          1   1 -0.033762   1  0.575268   71  3.000000        0         0         0\n",
            "724  99       7          0   0 -1.340497   1  0.575268   72  3.083333        1         0         0\n",
            "\n",
            "Variable Types:\n",
            "id             int64\n",
            "period         int64\n",
            "treatment      int64\n",
            "x1             int64\n",
            "x2           float64\n",
            "x3             int64\n",
            "x4           float64\n",
            "age            int64\n",
            "age_s        float64\n",
            "outcome        int64\n",
            "censored       int64\n",
            "eligible       int64\n",
            "\n",
            "## IPW for informative censoring:\n",
            " - No weight model specified\n",
            "\n",
            "## Sequence of Trials Data:\n",
            " - Use set_expansion_options() and expand_trials() to construct the sequence of trials dataset.\n",
            "\n",
            "## Outcome model:\n",
            " - Not specified\n",
            "##################################################\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def trial_sequence(estimand):\n",
        "    \"\"\"Initialize a trial sequence with specified estimand.\"\"\"\n",
        "    return {\n",
        "        \"estimand\": estimand,\n",
        "        \"data\": None,\n",
        "        \"ipw_model\": None,\n",
        "        \"outcome_model\": None\n",
        "    }\n",
        "\n",
        "def set_data(trial, data, id_col, period_col, treatment_col, outcome_col, eligible_col):\n",
        "    \"\"\"Prepare and store trial data with relevant columns.\"\"\"\n",
        "    trial[\"data\"] = data[\n",
        "        [id_col, period_col, treatment_col, \"x1\", \"x2\", \"x3\", \"x4\",\n",
        "         \"age\", \"age_s\", outcome_col, \"censored\", eligible_col]\n",
        "    ].copy()\n",
        "    return trial\n",
        "\n",
        "def summarize_trial(trial):\n",
        "    \"\"\"Print a structured summary of the trial object.\"\"\"\n",
        "    print(f\"## Trial Sequence Object\")\n",
        "    print(f\"## Estimand: {trial['estimand']}\\n\")\n",
        "\n",
        "    print(\"## Data:\")\n",
        "    data = trial[\"data\"]\n",
        "    if data is not None:\n",
        "        n_obs = len(data)\n",
        "        n_patients = data[\"id\"].nunique()\n",
        "        print(f\" - N: {n_obs} observations from {n_patients} patients\")\n",
        "\n",
        "        # Data preview with head and tail\n",
        "        print(\"\\nData Preview:\")\n",
        "        with pd.option_context('display.max_columns', None, 'display.expand_frame_repr', False):\n",
        "            print(data.head(2).to_string())\n",
        "            print(\"---\")\n",
        "            print(data.tail(2).to_string())\n",
        "\n",
        "        # Variable types\n",
        "        print(\"\\nVariable Types:\")\n",
        "        print(data.dtypes.to_string())\n",
        "    else:\n",
        "        print(\" - No data loaded\")\n",
        "\n",
        "    # IPW information\n",
        "    print(\"\\n## IPW for informative censoring:\")\n",
        "    print(\" - No weight model specified\" if trial[\"ipw_model\"] is None\n",
        "          else f\" - Model: {trial['ipw_model']}\")\n",
        "\n",
        "    # Trial sequence instructions\n",
        "    print(\"\\n## Sequence of Trials Data:\")\n",
        "    print(\" - Use set_expansion_options() and expand_trials() to construct the sequence of trials dataset.\")\n",
        "\n",
        "    # Outcome model information\n",
        "    print(\"\\n## Outcome model:\")\n",
        "    print(\" - Not specified\" if trial[\"outcome_model\"] is None\n",
        "          else f\" - Model: {trial['outcome_model']}\")\n",
        "\n",
        "# Initialize target trials\n",
        "trial_pp = trial_sequence(estimand=\"PP\")\n",
        "trial_itt = trial_sequence(estimand=\"ITT\")\n",
        "\n",
        "# Create output directories\n",
        "for dir_path in [os.path.join(os.getcwd(), \"trial_pp\"),\n",
        "                 os.path.join(os.getcwd(), \"trial_itt\")]:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "# Load and prepare data\n",
        "data_censored = pd.read_csv(\"data_censored.csv\")\n",
        "\n",
        "# Set up trials\n",
        "trial_pp = set_data(trial_pp, data_censored, \"id\", \"period\", \"treatment\", \"outcome\", \"eligible\")\n",
        "trial_itt = set_data(trial_itt, data_censored, \"id\", \"period\", \"treatment\", \"outcome\", \"eligible\")\n",
        "\n",
        "# Display ITT trial summary\n",
        "print(\"\\n\" + \"#\"*50)\n",
        "summarize_trial(trial_itt)\n",
        "print(\"#\"*50 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVA95vgm-m0B"
      },
      "source": [
        "# 3.1 Censoring due to treatment switching\n",
        "\n",
        "In the R code, the ```set_switch_weight_model()``` function is used to handle censoring due to treatment switching. This involves specifying numerator and denominator models to calculate the probability of receiving treatment in the current period, with separate models for patients based on their previous treatment status.\n",
        "\n",
        "To replicate this in Python, we can use logistic regression models from the ```statsmodels```library to estimate these probabilities. Here's how you can implement this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDaYb90k7BFA",
        "outputId": "444c9f6e-9626-4c0f-e96f-01f9ce7f4142"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                           Logit Regression Results                           \n",
            "==============================================================================\n",
            "Dep. Variable:              treatment   No. Observations:                  725\n",
            "Model:                          Logit   Df Residuals:                      723\n",
            "Method:                           MLE   Df Model:                            1\n",
            "Date:                Wed, 05 Mar 2025   Pseudo R-squ.:                 0.04144\n",
            "Time:                        15:27:35   Log-Likelihood:                -480.24\n",
            "converged:                       True   LL-Null:                       -501.01\n",
            "Covariance Type:            nonrobust   LLR p-value:                 1.163e-10\n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept      1.8867      0.333      5.671      0.000       1.235       2.539\n",
            "age           -0.0421      0.007     -6.213      0.000      -0.055      -0.029\n",
            "==============================================================================\n",
            "                           Logit Regression Results                           \n",
            "==============================================================================\n",
            "Dep. Variable:              treatment   No. Observations:                  725\n",
            "Model:                          Logit   Df Residuals:                      721\n",
            "Method:                           MLE   Df Model:                            3\n",
            "Date:                Wed, 05 Mar 2025   Pseudo R-squ.:                 0.04459\n",
            "Time:                        15:27:35   Log-Likelihood:                -478.67\n",
            "converged:                       True   LL-Null:                       -501.01\n",
            "Covariance Type:            nonrobust   LLR p-value:                 1.084e-09\n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept      1.8308      0.356      5.145      0.000       1.133       2.528\n",
            "age           -0.0429      0.007     -6.261      0.000      -0.056      -0.029\n",
            "x1             0.2744      0.157      1.752      0.080      -0.033       0.581\n",
            "x3            -0.0321      0.155     -0.207      0.836      -0.336       0.272\n",
            "==============================================================================\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Step 1: We define a helper function fit_logistic_regression to fit logistic regression models using statsmodels.\n",
        "Step 2: The set_switch_weight_model function fits the numerator and denominator models based on the provided formulas and stores them in the trial object.\n",
        "Step 3: We apply this function to the per-protocol trial (trial_pp) with the specified formulas.\n",
        "'''\n",
        "\n",
        "\n",
        "#Step 1\n",
        "def fit_logistic_regression(formula, data):\n",
        "    \"\"\"Fit a logistic regression model.\"\"\"\n",
        "    model = sm.Logit.from_formula(formula, data)\n",
        "    result = model.fit(disp=False)\n",
        "    return result\n",
        "\n",
        "#Step 2\n",
        "def set_switch_weight_model(trial, numerator_formula, denominator_formula, save_path=None):\n",
        "    \"\"\"\n",
        "    Set up the switch weight models for treatment switching.\n",
        "    \"\"\"\n",
        "    data = trial[\"data\"].copy()\n",
        "\n",
        "    # Fit numerator model\n",
        "    numerator_model = fit_logistic_regression(numerator_formula, data)\n",
        "\n",
        "    # Fit denominator model\n",
        "    denominator_model = fit_logistic_regression(denominator_formula, data)\n",
        "\n",
        "    # Store models in the trial object\n",
        "    trial[\"switch_weights\"] = {\n",
        "        \"numerator_model\": numerator_model,\n",
        "        \"denominator_model\": denominator_model\n",
        "    }\n",
        "\n",
        "    # Optionally, save the models\n",
        "    if save_path:\n",
        "        # Create the directory if it doesn't exist\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        numerator_model.save(f\"{save_path}/numerator_model.pickle\")\n",
        "        denominator_model.save(f\"{save_path}/denominator_model.pickle\")\n",
        "\n",
        "    return trial\n",
        "\n",
        "# Define formulas\n",
        "numerator_formula = 'treatment ~ age'\n",
        "denominator_formula = 'treatment ~ age + x1 + x3'\n",
        "\n",
        "# Apply to the per-protocol trial\n",
        "trial_pp = set_switch_weight_model(\n",
        "    trial_pp,\n",
        "    numerator_formula=numerator_formula,\n",
        "    denominator_formula=denominator_formula,\n",
        "    save_path='trial_pp/switch_models'\n",
        ")\n",
        "\n",
        "# Display switch weight models summary\n",
        "print(trial_pp[\"switch_weights\"][\"numerator_model\"].summary())\n",
        "print(trial_pp[\"switch_weights\"][\"denominator_model\"].summary())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHsDwHV8_eAl"
      },
      "source": [
        "# Step 3.2: Other Informative Censoring\n",
        "\n",
        "For other types of informative censoring, the R code uses the ```set_censor_weight_model()``` function to estimate inverse probability of censoring weights (IPCW). In Python, we can similarly fit logistic regression models to estimate the probability of censoring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBrx9ZU9_7Oq",
        "outputId": "b801545f-1261-4335-db1a-33bd68cf509e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                           Logit Regression Results                           \n",
            "==============================================================================\n",
            "Dep. Variable:           not_censored   No. Observations:                  725\n",
            "Model:                          Logit   Df Residuals:                      723\n",
            "Method:                           MLE   Df Model:                            1\n",
            "Date:                Wed, 05 Mar 2025   Pseudo R-squ.:                 0.02676\n",
            "Time:                        15:27:40   Log-Likelihood:                -196.70\n",
            "converged:                       True   LL-Null:                       -202.11\n",
            "Covariance Type:            nonrobust   LLR p-value:                  0.001007\n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept      2.4481      0.141     17.415      0.000       2.173       2.724\n",
            "x2            -0.4486      0.137     -3.278      0.001      -0.717      -0.180\n",
            "==============================================================================\n",
            "                           Logit Regression Results                           \n",
            "==============================================================================\n",
            "Dep. Variable:           not_censored   No. Observations:                  725\n",
            "Model:                          Logit   Df Residuals:                      722\n",
            "Method:                           MLE   Df Model:                            2\n",
            "Date:                Wed, 05 Mar 2025   Pseudo R-squ.:                 0.04069\n",
            "Time:                        15:27:40   Log-Likelihood:                -193.88\n",
            "converged:                       True   LL-Null:                       -202.11\n",
            "Covariance Type:            nonrobust   LLR p-value:                 0.0002679\n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept      2.2059      0.165     13.339      0.000       1.882       2.530\n",
            "x2            -0.4706      0.137     -3.423      0.001      -0.740      -0.201\n",
            "x1             0.7019      0.307      2.285      0.022       0.100       1.304\n",
            "==============================================================================\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Step 1: We add a not_censored column to indicate whether an observation is not censored.\n",
        "Step 2: The set_censor_weight_model function fits numerator and denominator models to estimate the probability of not being censored, based on the provided formulas.\n",
        "Step 3: We apply this function to both the per-protocol (trial_pp) and intention-to-treat (trial_itt) trials.\n",
        "'''\n",
        "\n",
        "def set_censor_weight_model(trial, censor_event_col, numerator_formula, denominator_formula, save_path=None):\n",
        "    \"\"\"\n",
        "    Set up the censor weight models for informative censoring.\n",
        "    \"\"\"\n",
        "    data = trial[\"data\"].copy()\n",
        "\n",
        "    # Create the censoring indicator (1 - censored)\n",
        "    data['not_censored'] = 1 - data[censor_event_col]\n",
        "\n",
        "    # Fit numerator model\n",
        "    numerator_model = fit_logistic_regression(numerator_formula, data)\n",
        "\n",
        "    # Fit denominator model\n",
        "    denominator_model = fit_logistic_regression(denominator_formula, data)\n",
        "\n",
        "    # Store models in the trial object\n",
        "    trial[\"censor_weights\"] = {\n",
        "        \"numerator_model\": numerator_model,\n",
        "        \"denominator_model\": denominator_model\n",
        "    }\n",
        "\n",
        "    # Optionally, save the models\n",
        "    if save_path:\n",
        "        # Ensure the directory exists before saving.\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        numerator_model.save(os.path.join(save_path, \"numerator_model.pickle\")) # Use os.path.join to create the full file path\n",
        "        denominator_model.save(os.path.join(save_path, \"denominator_model.pickle\"))  # Use os.path.join to create the full file path\n",
        "\n",
        "    return trial\n",
        "\n",
        "# Define formulas\n",
        "numerator_formula = 'not_censored ~ x2'\n",
        "denominator_formula = 'not_censored ~ x2 + x1'\n",
        "\n",
        "# Apply to the per-protocol trial\n",
        "trial_pp = set_censor_weight_model(\n",
        "    trial_pp,\n",
        "    censor_event_col='censored',\n",
        "    numerator_formula=numerator_formula,\n",
        "    denominator_formula=denominator_formula,\n",
        "    save_path='trial_pp/censor_models'\n",
        ")\n",
        "\n",
        "# Apply to the intention-to-treat trial\n",
        "trial_itt = set_censor_weight_model(\n",
        "    trial_itt,\n",
        "    censor_event_col='censored',\n",
        "    numerator_formula=numerator_formula,\n",
        "    denominator_formula=denominator_formula,\n",
        "    save_path='trial_itt/censor_models'\n",
        ")\n",
        "\n",
        "# Display censor weight models summary for ITT trial\n",
        "print(trial_itt[\"censor_weights\"][\"numerator_model\"].summary())\n",
        "print(trial_itt[\"censor_weights\"][\"denominator_model\"].summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPw94_onAhpS"
      },
      "source": [
        "# Step 4: Calculate Weights\n",
        "\n",
        "In the R code, the ```calculate_weights()``` function is used to fit the models specified earlier and compute the inverse probability weights. To replicate this functionality in Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ld40cliBUbF",
        "outputId": "10056c11-fa08-4e55-c5ea-392a8a537041"
      },
      "outputs": [
        {
          "ename": "PatsyError",
          "evalue": "Error evaluating factor: NameError: name 'not_censored' is not defined\n    not_censored ~ x2 + x1\n    ^^^^^^^^^^^^",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/patsy/compat.py:40\u001b[0m, in \u001b[0;36mcall_and_wrap_exc\u001b[0;34m(msg, origin, f, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/patsy/eval.py:179\u001b[0m, in \u001b[0;36mEvalEnvironment.eval\u001b[0;34m(self, expr, source_name, inner_namespace)\u001b[0m\n\u001b[1;32m    178\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcompile\u001b[39m(expr, source_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflags, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVarLookupDict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minner_namespace\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_namespaces\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m<string>:1\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'not_censored' is not defined",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mPatsyError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 121\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(den_model\u001b[38;5;241m.\u001b[39msummary())\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Apply weight calculation\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m trial_pp \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial_pp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m trial_itt \u001b[38;5;241m=\u001b[39m calculate_weights(trial_itt)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Display weight model summaries\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[6], line 31\u001b[0m, in \u001b[0;36mcalculate_weights\u001b[0;34m(trial, switch_save_path, censor_save_path)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Denominator probability (fit separate models for prev_treatment = 0 and 1)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m den_formula \u001b[38;5;241m=\u001b[39m denominator_formula \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdenominator_formula\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot_censored ~ x2 + x1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 31\u001b[0m den_model_subset \u001b[38;5;241m=\u001b[39m \u001b[43msm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLogit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_formula\u001b[49m\u001b[43m(\u001b[49m\u001b[43mden_formula\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_subset\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfit(disp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Save denominator model\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m censor_save_path:\n",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/statsmodels/base/model.py:205\u001b[0m, in \u001b[0;36mModel.from_formula\u001b[0;34m(cls, formula, data, subset, drop_cols, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m:  \u001b[38;5;66;03m# with patsy it's drop or raise. let's raise.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     missing \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 205\u001b[0m tmp \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_formula_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformula\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m ((endog, exog), missing_idx, model_spec) \u001b[38;5;241m=\u001b[39m tmp\n\u001b[1;32m    208\u001b[0m max_endog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_formula_max_endog\n",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/statsmodels/formula/formulatools.py:54\u001b[0m, in \u001b[0;36mhandle_formula_data\u001b[0;34m(Y, X, formula, depth, missing)\u001b[0m\n\u001b[1;32m     45\u001b[0m     result \u001b[38;5;241m=\u001b[39m mgr\u001b[38;5;241m.\u001b[39mget_matrices(\n\u001b[1;32m     46\u001b[0m         formula,\n\u001b[1;32m     47\u001b[0m         (Y, X),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m         attach_spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     52\u001b[0m     )\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mmgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_matrices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mformula\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpandas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m missing_mask \u001b[38;5;241m=\u001b[39m mgr\u001b[38;5;241m.\u001b[39mmissing_mask\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(missing_mask):\n",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/statsmodels/formula/_manager.py:463\u001b[0m, in \u001b[0;36mFormulaManager.get_matrices\u001b[0;34m(self, formula, data, eval_env, pandas, na_action, prediction)\u001b[0m\n\u001b[1;32m    459\u001b[0m     output \u001b[38;5;241m=\u001b[39m patsy\u001b[38;5;241m.\u001b[39mdmatrix(\n\u001b[1;32m    460\u001b[0m         formula, data, eval_env\u001b[38;5;241m=\u001b[39meval_env, return_type\u001b[38;5;241m=\u001b[39mreturn_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    461\u001b[0m     )\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# \"~\" in formula:\u001b[39;00m\n\u001b[0;32m--> 463\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mpatsy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdmatrices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mformula\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spec \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdesign_info\n",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/patsy/highlevel.py:319\u001b[0m, in \u001b[0;36mdmatrices\u001b[0;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct two design matrices given a formula_like and data.\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \n\u001b[1;32m    311\u001b[0m \u001b[38;5;124;03mThis function is identical to :func:`dmatrix`, except that it requires\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03mSee :func:`dmatrix` for details.\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    318\u001b[0m eval_env \u001b[38;5;241m=\u001b[39m EvalEnvironment\u001b[38;5;241m.\u001b[39mcapture(eval_env, reference\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 319\u001b[0m (lhs, rhs) \u001b[38;5;241m=\u001b[39m \u001b[43m_do_highlevel_design\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformula_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNA_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_type\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lhs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PatsyError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel is missing required outcome variables\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/patsy/highlevel.py:164\u001b[0m, in \u001b[0;36m_do_highlevel_design\u001b[0;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdata_iter_maker\u001b[39m():\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m([data])\n\u001b[0;32m--> 164\u001b[0m design_infos \u001b[38;5;241m=\u001b[39m \u001b[43m_try_incr_builders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformula_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_iter_maker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNA_action\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m design_infos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m build_design_matrices(\n\u001b[1;32m    169\u001b[0m         design_infos, data, NA_action\u001b[38;5;241m=\u001b[39mNA_action, return_type\u001b[38;5;241m=\u001b[39mreturn_type\n\u001b[1;32m    170\u001b[0m     )\n",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/patsy/highlevel.py:56\u001b[0m, in \u001b[0;36m_try_incr_builders\u001b[0;34m(formula_like, data_iter_maker, eval_env, NA_action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formula_like, ModelDesc):\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eval_env, EvalEnvironment)\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdesign_matrix_builders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mformula_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlhs_termlist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformula_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrhs_termlist\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_iter_maker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mNA_action\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/patsy/build.py:746\u001b[0m, in \u001b[0;36mdesign_matrix_builders\u001b[0;34m(termlists, data_iter_maker, eval_env, NA_action)\u001b[0m\n\u001b[1;32m    743\u001b[0m factor_states \u001b[38;5;241m=\u001b[39m _factors_memorize(all_factors, data_iter_maker, eval_env)\n\u001b[1;32m    744\u001b[0m \u001b[38;5;66;03m# Now all the factors have working eval methods, so we can evaluate them\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;66;03m# on some data to find out what type of data they return.\u001b[39;00m\n\u001b[0;32m--> 746\u001b[0m (num_column_counts, cat_levels_contrasts) \u001b[38;5;241m=\u001b[39m \u001b[43m_examine_factor_types\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_factors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_iter_maker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNA_action\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Now we need the factor infos, which encapsulate the knowledge of\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;66;03m# how to turn any given factor into a chunk of data:\u001b[39;00m\n\u001b[1;32m    751\u001b[0m factor_infos \u001b[38;5;241m=\u001b[39m {}\n",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/patsy/build.py:491\u001b[0m, in \u001b[0;36m_examine_factor_types\u001b[0;34m(factors, factor_states, data_iter_maker, NA_action)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m data_iter_maker():\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m factor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(examine_needed):\n\u001b[0;32m--> 491\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mfactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfactor_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m factor \u001b[38;5;129;01min\u001b[39;00m cat_sniffers \u001b[38;5;129;01mor\u001b[39;00m guess_categorical(value):\n\u001b[1;32m    493\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m factor \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m cat_sniffers:\n",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/patsy/eval.py:599\u001b[0m, in \u001b[0;36mEvalFactor.eval\u001b[0;34m(self, memorize_state, data)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21meval\u001b[39m(\u001b[38;5;28mself\u001b[39m, memorize_state, data):\n\u001b[0;32m--> 599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemorize_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_code\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemorize_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/patsy/eval.py:582\u001b[0m, in \u001b[0;36mEvalFactor._eval\u001b[0;34m(self, code, memorize_state, data)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_eval\u001b[39m(\u001b[38;5;28mself\u001b[39m, code, memorize_state, data):\n\u001b[1;32m    581\u001b[0m     inner_namespace \u001b[38;5;241m=\u001b[39m VarLookupDict([data, memorize_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransforms\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[0;32m--> 582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_and_wrap_exc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mError evaluating factor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemorize_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_env\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minner_namespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_namespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/patsy/compat.py:43\u001b[0m, in \u001b[0;36mcall_and_wrap_exc\u001b[0;34m(msg, origin, f, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     42\u001b[0m     new_exc \u001b[38;5;241m=\u001b[39m PatsyError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (msg, e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, e), origin)\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
            "\u001b[0;31mPatsyError\u001b[0m: Error evaluating factor: NameError: name 'not_censored' is not defined\n    not_censored ~ x2 + x1\n    ^^^^^^^^^^^^"
          ]
        }
      ],
      "source": [
        "\n",
        "def calculate_weights(trial, switch_save_path='switch_models', censor_save_path='censor_models'):\n",
        "    \"\"\"\n",
        "    Calculate stabilized inverse probability weights for censoring and treatment switching.\n",
        "    \"\"\"\n",
        "    data = trial[\"data\"].copy()\n",
        "    \n",
        "    # Add previous treatment column\n",
        "    data['prev_treatment'] = data.groupby('id')['treatment'].shift(1)\n",
        "    data['prev_treatment'] = data['prev_treatment'].fillna(0)  # Assume first period has no previous treatment\n",
        "    \n",
        "    # Initialize weight columns\n",
        "    data['censor_weight'] = 1.0\n",
        "    data['switch_weight'] = 1.0\n",
        "    \n",
        "    # Calculate censoring weights\n",
        "    if \"censor_weights\" in trial:\n",
        "        num_model = trial[\"censor_weights\"][\"numerator_model\"]\n",
        "        den_model = trial[\"censor_weights\"][\"denominator_model\"]\n",
        "        \n",
        "        # Split data by previous treatment\n",
        "        for prev_treat in [0, 1]:\n",
        "            mask = data['prev_treatment'] == prev_treat\n",
        "            data_subset = data[mask].copy()\n",
        "            \n",
        "            if len(data_subset) > 0:\n",
        "                # Numerator probability\n",
        "                p_num = num_model.predict(data_subset)\n",
        "                \n",
        "                # Denominator probability (fit separate models for prev_treatment = 0 and 1)\n",
        "                den_formula = denominator_formula if 'denominator_formula' in locals() else 'not_censored ~ x2 + x1'\n",
        "                den_model_subset = sm.Logit.from_formula(den_formula, data_subset).fit(disp=False)\n",
        "                \n",
        "                # Save denominator model\n",
        "                if censor_save_path:\n",
        "                    save_dir = os.path.join(os.getcwd(), trial_itt['estimand'].lower(), censor_save_path)\n",
        "                    os.makedirs(save_dir, exist_ok=True)\n",
        "                    model_path = os.path.join(save_dir, f\"model_d{prev_treat}.pickle\")\n",
        "                    den_model_subset.save(model_path)\n",
        "                \n",
        "                p_den = den_model_subset.predict(data_subset)\n",
        "                \n",
        "                # Calculate stabilized weights\n",
        "                weights = p_num / p_den\n",
        "                data.loc[mask, 'censor_weight'] = weights.clip(lower=0, upper=10)  # Truncate extreme weights\n",
        "    \n",
        "    # Calculate switching weights (only for PP trial)\n",
        "    if trial[\"estimand\"] == \"PP\" and \"switch_weights\" in trial:\n",
        "        num_model = trial[\"switch_weights\"][\"numerator_model\"]\n",
        "        den_model = trial[\"switch_weights\"][\"denominator_model\"]\n",
        "        \n",
        "        for prev_treat in [0, 1]:\n",
        "            mask = data['prev_treatment'] == prev_treat\n",
        "            data_subset = data[mask].copy()\n",
        "            \n",
        "            if len(data_subset) > 0:\n",
        "                # Numerator probability\n",
        "                p_num = num_model.predict(data_subset)\n",
        "                \n",
        "                # Denominator probability (fit separate models for prev_treatment = 0 and 1)\n",
        "                den_formula = denominator_formula if 'denominator_formula' in locals() else 'treatment ~ age + x1 + x3'\n",
        "                den_model_subset = sm.Logit.from_formula(den_formula, data_subset).fit(disp=False)\n",
        "                \n",
        "                # Save denominator model\n",
        "                if switch_save_path:\n",
        "                    save_dir = os.path.join(os.getcwd(), trial_pp['estimand'].lower(), switch_save_path)\n",
        "                    os.makedirs(save_dir, exist_ok=True)\n",
        "                    model_path = os.path.join(save_dir, f\"model_d{prev_treat}.pickle\")\n",
        "                    den_model_subset.save(model_path)\n",
        "                \n",
        "                p_den = den_model_subset.predict(data_subset)\n",
        "                \n",
        "                # Calculate stabilized weights\n",
        "                weights = p_num / p_den\n",
        "                data.loc[mask, 'switch_weight'] = weights.clip(lower=0, upper=10)  # Truncate extreme weights\n",
        "    \n",
        "    # Update trial data\n",
        "    trial[\"data\"] = data\n",
        "    \n",
        "    return trial\n",
        "\n",
        "def show_weight_models(trial):\n",
        "    \"\"\"Display weight model summaries.\"\"\"\n",
        "    if trial[\"estimand\"] == \"PP\":\n",
        "        print(\"Weight Models for Treatment Switching\")\n",
        "        print(\"-------------------------------------\")\n",
        "        \n",
        "        # Switching weights\n",
        "        if \"switch_weights\" in trial:\n",
        "            for prev_treat in [0, 1]:\n",
        "                print(f\"\\n[[n{prev_treat}]]\")\n",
        "                print(f\"Model: P(treatment = 1 | previous treatment = {prev_treat}) for numerator\")\n",
        "                print(trial[\"switch_weights\"][\"numerator_model\"].summary())\n",
        "                \n",
        "                print(f\"\\n[[d{prev_treat}]]\")\n",
        "                print(f\"Model: P(treatment = 1 | previous treatment = {prev_treat}) for denominator\")\n",
        "                model_path = os.path.join(os.getcwd(), trial['estimand'].lower(), \n",
        "                                       'switch_models', f\"model_d{prev_treat}.pickle\")\n",
        "                if os.path.exists(model_path):\n",
        "                    den_model = sm.Logit.load(model_path)\n",
        "                    print(den_model.summary())\n",
        "    \n",
        "    print(\"\\nWeight Models for Informative Censoring\")\n",
        "    print(\"---------------------------------------\")\n",
        "    \n",
        "    # Censoring weights\n",
        "    if \"censor_weights\" in trial:\n",
        "        for prev_treat in [0, 1]:\n",
        "            print(f\"\\n[[n{prev_treat}]]\" if trial[\"estimand\"] == \"PP\" else \"\\n[[n]]\")\n",
        "            print(f\"Model: P(censor_event = 0 | X{', previous treatment = ' + str(prev_treat) if trial['estimand'] == 'PP' else ''}) for numerator\")\n",
        "            print(trial[\"censor_weights\"][\"numerator_model\"].summary())\n",
        "            \n",
        "            print(f\"\\n[[d{prev_treat}]]\")\n",
        "            print(f\"Model: P(censor_event = 0 | X, previous treatment = {prev_treat}) for denominator\")\n",
        "            model_path = os.path.join(os.getcwd(), trial['estimand'].lower(), \n",
        "                                   'censor_models', f\"model_d{prev_treat}.pickle\")\n",
        "            if os.path.exists(model_path):\n",
        "                den_model = sm.Logit.load(model_path)\n",
        "                print(den_model.summary())\n",
        "\n",
        "# Apply weight calculation\n",
        "trial_pp = calculate_weights(trial_pp)\n",
        "trial_itt = calculate_weights(trial_itt)\n",
        "\n",
        "# Display weight model summaries\n",
        "print(\"\\n\" + \"#\"*50)\n",
        "show_weight_models(trial_itt)\n",
        "print(\"#\"*50 + \"\\n\")\n",
        "show_weight_models(trial_pp)\n",
        "print(\"#\"*50 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuHgQOVzCfgU"
      },
      "source": [
        "# Step 5: Set Expansion Options\n",
        "\n",
        "In this step, we define how the trial data should be expanded by specifying parameters such as the output method and chunk size. This setup prepares the trial object for the expansion process.\n",
        "\n",
        "## ```set_expansion_options``` Function:\n",
        "Adds an ```expansion_options``` dictionary to the trial object, containing the output function and chunk size.\n",
        "\n",
        "## ```save_to_csv``` Function:\n",
        "Generates a function that saves each data chunk to a CSV file in the specified directory. This function is assigned to the ```output_func``` parameter in ```set_expansion_options```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PK2Rc14DDcG"
      },
      "outputs": [],
      "source": [
        "def set_expansion_options(trial, output_func, chunk_size=500):\n",
        "    \"\"\"\n",
        "    Set options for expanding the trial data.\n",
        "\n",
        "    Parameters:\n",
        "    - trial (dict): The trial object containing data and configurations.\n",
        "    - output_func (function): Function to process and save each data chunk.\n",
        "    - chunk_size (int): Number of patients to include in each expansion iteration.\n",
        "\n",
        "    Returns:\n",
        "    - dict: Updated trial object with expansion options set.\n",
        "    \"\"\"\n",
        "    trial[\"expansion_options\"] = {\n",
        "        \"output_func\": output_func,\n",
        "        \"chunk_size\": chunk_size\n",
        "    }\n",
        "    return trial\n",
        "\n",
        "def save_to_csv(directory):\n",
        "    \"\"\"\n",
        "    Creates a function to save DataFrame chunks to CSV files in the specified directory.\n",
        "\n",
        "    Parameters:\n",
        "    - directory (str): Directory path where CSV files will be saved.\n",
        "\n",
        "    Returns:\n",
        "    - function: A function that takes a DataFrame chunk and saves it as a CSV file.\n",
        "    \"\"\"\n",
        "    def process_chunk(chunk, chunk_id):\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "        file_path = os.path.join(directory, f\"expanded_data_chunk_{chunk_id}.csv\")\n",
        "        chunk.to_csv(file_path, index=False)\n",
        "        print(f\"Saved: {file_path}\")\n",
        "    return process_chunk\n",
        "\n",
        "# Set expansion options for the per-protocol trial\n",
        "trial_pp = set_expansion_options(\n",
        "    trial_pp,\n",
        "    output_func=save_to_csv(\"trial_pp/expanded_data\"),\n",
        "    chunk_size=500\n",
        ")\n",
        "\n",
        "# Set expansion options for the intention-to-treat trial\n",
        "trial_itt = set_expansion_options(\n",
        "    trial_itt,\n",
        "    output_func=save_to_csv(\"trial_itt/expanded_data\"),\n",
        "    chunk_size=500\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b55fBKHyCjBW"
      },
      "source": [
        "# Step 6: Expand Trials\n",
        "\n",
        "With the expansion options set, this step involves processing the trial data in chunks as specified, applying the output function to each chunk to generate the expanded dataset.\n",
        "\n",
        "##```expand_trials``` Function:\n",
        "Retrieves the data and expansion options from the trial object, divides the data into chunks based on the specified chunk size, and applies the output function to each chunk. The ```output_func``` is called with the data chunk and its corresponding chunk index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hl_3-BpA2qP"
      },
      "outputs": [],
      "source": [
        "def expand_trials(trial, censor_at_switch=True, first_period=0, last_period=10):\n",
        "    \"\"\"\n",
        "    Expand the trial data to emulate a sequence of target trials.\n",
        "\n",
        "    Parameters:\n",
        "    - trial (dict): The trial object containing the original data.\n",
        "    - censor_at_switch (bool): Whether to censor data at treatment switch.\n",
        "    - first_period (int): The starting period for the trials.\n",
        "    - last_period (int or float): The ending period for the trials.\n",
        "\n",
        "    Returns:\n",
        "    - dict: Updated trial object with expanded data.\n",
        "    \"\"\"\n",
        "    data = trial.get(\"data\")\n",
        "    if data is None:\n",
        "        raise ValueError(\"No data found in the trial object.\")\n",
        "\n",
        "    expanded_rows = []\n",
        "\n",
        "    for _, patient_data in data.groupby('id'):\n",
        "        patient_data = patient_data.sort_values(by='period')\n",
        "        start_age = patient_data['age'].iloc[0]\n",
        "        assigned_treatment = patient_data['treatment'].iloc[0]\n",
        "\n",
        "        for trial_period in range(first_period, int(last_period) + 1):\n",
        "            followup_time = 0\n",
        "            for _, row in patient_data.iterrows():\n",
        "                if censor_at_switch and row['treatment'] != assigned_treatment:\n",
        "                    break\n",
        "\n",
        "                expanded_row = row.copy()\n",
        "                expanded_row['trial_period'] = trial_period\n",
        "                expanded_row['followup_time'] = followup_time\n",
        "                expanded_row['assigned_treatment'] = assigned_treatment\n",
        "                expanded_rows.append(expanded_row)\n",
        "\n",
        "                followup_time += 1\n",
        "\n",
        "    expanded_data = pd.DataFrame(expanded_rows)\n",
        "    trial['expanded_data'] = expanded_data\n",
        "\n",
        "    return trial\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FncaIAlPEN99"
      },
      "source": [
        "# Step 6.1: Create Sequence of Trials Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJFG4Ef4EC0O",
        "outputId": "7e28a7a4-5056-453f-e785-7d2193c9e475"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    id  period  treatment   x1        x2   x3        x4   age     age_s  \\\n",
            "0  1.0     0.0        1.0  1.0  1.146148  0.0  0.734203  36.0  0.083333   \n",
            "1  1.0     1.0        1.0  1.0  0.002200  0.0  0.734203  37.0  0.166667   \n",
            "2  1.0     2.0        1.0  0.0 -0.481762  0.0  0.734203  38.0  0.250000   \n",
            "3  1.0     3.0        1.0  0.0  0.007872  0.0  0.734203  39.0  0.333333   \n",
            "4  1.0     4.0        1.0  1.0  0.216054  0.0  0.734203  40.0  0.416667   \n",
            "\n",
            "   outcome  ...  p_treatment_denom  treatment_weight  not_censored  \\\n",
            "0      0.0  ...           0.636513          0.930088           1.0   \n",
            "1      0.0  ...           0.626528          0.928634           1.0   \n",
            "2      0.0  ...           0.549849          1.039459           1.0   \n",
            "3      0.0  ...           0.539206          1.040816           1.0   \n",
            "4      0.0  ...           0.595948          0.924292           1.0   \n",
            "\n",
            "   p_not_censored_num  p_not_censored_denom  censoring_weight  ip_weight  \\\n",
            "0            0.873678              0.914385          0.955481   0.888682   \n",
            "1            0.920349              0.948181          0.970647   0.901376   \n",
            "2            0.934883              0.919281          1.016972   1.057101   \n",
            "3            0.920163              0.900444          1.021899   1.063608   \n",
            "4            0.913026              0.943007          0.968208   0.894907   \n",
            "\n",
            "   trial_period  followup_time  assigned_treatment  \n",
            "0           0.0            0.0                 1.0  \n",
            "1           0.0            1.0                 1.0  \n",
            "2           0.0            2.0                 1.0  \n",
            "3           0.0            3.0                 1.0  \n",
            "4           0.0            4.0                 1.0  \n",
            "\n",
            "[5 rows x 23 columns]\n"
          ]
        }
      ],
      "source": [
        "trial_pp = expand_trials(trial_pp)\n",
        "trial_itt = expand_trials(trial_itt)\n",
        "\n",
        "# Display the first few rows of the expanded data for the per-protocol trial\n",
        "print(trial_pp['expanded_data'].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPhhWmb3FWCL"
      },
      "source": [
        "# Step 7: Load or Sample from Expanded Data\n",
        "\n",
        "After expanding the trial data, we prepare it for fitting the outcome model. If the dataset is manageable in size, we can load it directly into memory. For larger datasets, sampling may be necessary to ensure efficient processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8rAcgOBGVlF"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "In this snippet:\n",
        "\n",
        "  We set a random seed for reproducibility.\n",
        "  We define the proportion (p_control) of control observations (where outcome == 0) to include in the sampled data.\n",
        "  We separate the treated and control observations.\n",
        "  We sample from the control observations based on the defined proportion.\n",
        "  We combine the treated observations with the sampled control observations to create the sampled_data DataFrame.\n",
        "'''\n",
        "\n",
        "# Assuming 'expanded_data' is your expanded DataFrame\n",
        "# For large datasets, sample a subset; for small datasets, you might skip this step\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "random_seed = 1234\n",
        "\n",
        "# Define the proportion of control observations to include\n",
        "p_control = 0.5\n",
        "\n",
        "# Access the 'expanded_data' from the trial dictionary\n",
        "expanded_data = trial_pp['expanded_data']  # or trial_itt['expanded_data'], depending on which trial you want to sample from\n",
        "\n",
        "# Separate treated and control observations\n",
        "treated = expanded_data[expanded_data['outcome'] == 1]\n",
        "control = expanded_data[expanded_data['outcome'] == 0]\n",
        "\n",
        "# Sample from control observations\n",
        "control_sampled = resample(control, replace=False, n_samples=int(p_control * len(control)), random_state=random_seed)\n",
        "\n",
        "# Combine treated observations with sampled control observations\n",
        "sampled_data = pd.concat([treated, control_sampled])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD2YVfB5Gmr4"
      },
      "source": [
        "# Step 8: Fit Marginal Structural Model\n",
        "\n",
        "To fit an MSM, we utilize inverse probability weighting (IPW). The ```zEpid``` library provides tools to calculate these weights and fit the MSM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "Hkl4zdd8GJYe",
        "outputId": "30c8c192-0253-44c6-83e8-110a88681995"
      },
      "outputs": [
        {
          "ename": "PatsyError",
          "evalue": "Error evaluating factor: NameError: name 'trial_period' is not defined\n    treatment ~ x2 + followup_time + I(followup_time**2) + trial_period + I(trial_period**2)\n                                                           ^^^^^^^^^^^^",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/patsy/compat.py:40\u001b[0m, in \u001b[0;36mcall_and_wrap_exc\u001b[0;34m(msg, origin, f, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/patsy/eval.py:179\u001b[0m, in \u001b[0;36mEvalEnvironment.eval\u001b[0;34m(self, expr, source_name, inner_namespace)\u001b[0m\n\u001b[1;32m    178\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcompile\u001b[39m(expr, source_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflags, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVarLookupDict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minner_namespace\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_namespaces\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m<string>:1\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trial_period' is not defined",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mPatsyError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Specify the exposure (treatment assignment) model\u001b[39;00m\n\u001b[1;32m     18\u001b[0m iptw \u001b[38;5;241m=\u001b[39m IPTW(df, treatment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtreatment\u001b[39m\u001b[38;5;124m'\u001b[39m, outcome\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutcome\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[43miptw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtreatment_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx2 + followup_time + I(followup_time**2) + trial_period + I(trial_period**2)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Specify the outcome model (MSM)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m iptw\u001b[38;5;241m.\u001b[39mmarginal_structural_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124massigned_treatment\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/zepid/causal/ipw/IPTW.py:222\u001b[0m, in \u001b[0;36mIPTW.treatment_model\u001b[0;34m(self, model_denominator, model_numerator, stabilized, bound, print_results)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# Calculating denominator probabilities\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__mdenom \u001b[38;5;241m=\u001b[39m model_denominator\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__denom__\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__numer__\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miptw \u001b[38;5;241m=\u001b[39m \u001b[43miptw_calculator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mtreatment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtreatment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mmodel_denom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_denominator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mmodel_numer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_numerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_weight_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mstabilized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstabilized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mstandardize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstandardize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mprint_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_results\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/zepid/causal/utils.py:304\u001b[0m, in \u001b[0;36miptw_calculator\u001b[0;34m(df, treatment, model_denom, model_numer, weight, stabilized, standardize, bound, print_results)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21miptw_calculator\u001b[39m(df, treatment, model_denom, model_numer, weight, stabilized, standardize, bound, print_results):\n\u001b[1;32m    301\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Background function to calculate inverse probability of treatment weights. Used by `IPTW`, `AIPTW`, `IPSW`,\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;124;03m    `AIPSW`\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m     denominator_model \u001b[38;5;241m=\u001b[39m \u001b[43mpropensity_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreatment\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m ~ \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_denom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m     d \u001b[38;5;241m=\u001b[39m denominator_model\u001b[38;5;241m.\u001b[39mpredict(df)\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;66;03m# Calculating numerator probabilities (if stabilized)\u001b[39;00m\n",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/zepid/causal/utils.py:114\u001b[0m, in \u001b[0;36mpropensity_score\u001b[0;34m(df, model, weights, print_results)\u001b[0m\n\u001b[1;32m    112\u001b[0m f \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mfamilies\u001b[38;5;241m.\u001b[39mfamily\u001b[38;5;241m.\u001b[39mBinomial()\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     log \u001b[38;5;241m=\u001b[39m \u001b[43msmf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfit()\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     log \u001b[38;5;241m=\u001b[39m smf\u001b[38;5;241m.\u001b[39mglm(model, df, freq_weights\u001b[38;5;241m=\u001b[39mdf[weights], family\u001b[38;5;241m=\u001b[39mf)\u001b[38;5;241m.\u001b[39mfit()\n",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/statsmodels/base/model.py:205\u001b[0m, in \u001b[0;36mModel.from_formula\u001b[0;34m(cls, formula, data, subset, drop_cols, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m:  \u001b[38;5;66;03m# with patsy it's drop or raise. let's raise.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     missing \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 205\u001b[0m tmp \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_formula_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformula\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m ((endog, exog), missing_idx, model_spec) \u001b[38;5;241m=\u001b[39m tmp\n\u001b[1;32m    208\u001b[0m max_endog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_formula_max_endog\n",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/statsmodels/formula/formulatools.py:54\u001b[0m, in \u001b[0;36mhandle_formula_data\u001b[0;34m(Y, X, formula, depth, missing)\u001b[0m\n\u001b[1;32m     45\u001b[0m     result \u001b[38;5;241m=\u001b[39m mgr\u001b[38;5;241m.\u001b[39mget_matrices(\n\u001b[1;32m     46\u001b[0m         formula,\n\u001b[1;32m     47\u001b[0m         (Y, X),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m         attach_spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     52\u001b[0m     )\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mmgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_matrices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mformula\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpandas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m missing_mask \u001b[38;5;241m=\u001b[39m mgr\u001b[38;5;241m.\u001b[39mmissing_mask\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(missing_mask):\n",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/statsmodels/formula/_manager.py:463\u001b[0m, in \u001b[0;36mFormulaManager.get_matrices\u001b[0;34m(self, formula, data, eval_env, pandas, na_action, prediction)\u001b[0m\n\u001b[1;32m    459\u001b[0m     output \u001b[38;5;241m=\u001b[39m patsy\u001b[38;5;241m.\u001b[39mdmatrix(\n\u001b[1;32m    460\u001b[0m         formula, data, eval_env\u001b[38;5;241m=\u001b[39meval_env, return_type\u001b[38;5;241m=\u001b[39mreturn_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    461\u001b[0m     )\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# \"~\" in formula:\u001b[39;00m\n\u001b[0;32m--> 463\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mpatsy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdmatrices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mformula\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spec \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdesign_info\n",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/patsy/highlevel.py:319\u001b[0m, in \u001b[0;36mdmatrices\u001b[0;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct two design matrices given a formula_like and data.\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \n\u001b[1;32m    311\u001b[0m \u001b[38;5;124;03mThis function is identical to :func:`dmatrix`, except that it requires\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03mSee :func:`dmatrix` for details.\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    318\u001b[0m eval_env \u001b[38;5;241m=\u001b[39m EvalEnvironment\u001b[38;5;241m.\u001b[39mcapture(eval_env, reference\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 319\u001b[0m (lhs, rhs) \u001b[38;5;241m=\u001b[39m \u001b[43m_do_highlevel_design\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformula_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNA_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_type\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lhs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PatsyError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel is missing required outcome variables\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/patsy/highlevel.py:164\u001b[0m, in \u001b[0;36m_do_highlevel_design\u001b[0;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdata_iter_maker\u001b[39m():\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m([data])\n\u001b[0;32m--> 164\u001b[0m design_infos \u001b[38;5;241m=\u001b[39m \u001b[43m_try_incr_builders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformula_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_iter_maker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNA_action\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m design_infos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m build_design_matrices(\n\u001b[1;32m    169\u001b[0m         design_infos, data, NA_action\u001b[38;5;241m=\u001b[39mNA_action, return_type\u001b[38;5;241m=\u001b[39mreturn_type\n\u001b[1;32m    170\u001b[0m     )\n",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/patsy/highlevel.py:56\u001b[0m, in \u001b[0;36m_try_incr_builders\u001b[0;34m(formula_like, data_iter_maker, eval_env, NA_action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formula_like, ModelDesc):\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eval_env, EvalEnvironment)\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdesign_matrix_builders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mformula_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlhs_termlist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformula_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrhs_termlist\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_iter_maker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mNA_action\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/patsy/build.py:746\u001b[0m, in \u001b[0;36mdesign_matrix_builders\u001b[0;34m(termlists, data_iter_maker, eval_env, NA_action)\u001b[0m\n\u001b[1;32m    743\u001b[0m factor_states \u001b[38;5;241m=\u001b[39m _factors_memorize(all_factors, data_iter_maker, eval_env)\n\u001b[1;32m    744\u001b[0m \u001b[38;5;66;03m# Now all the factors have working eval methods, so we can evaluate them\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;66;03m# on some data to find out what type of data they return.\u001b[39;00m\n\u001b[0;32m--> 746\u001b[0m (num_column_counts, cat_levels_contrasts) \u001b[38;5;241m=\u001b[39m \u001b[43m_examine_factor_types\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_factors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_iter_maker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNA_action\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Now we need the factor infos, which encapsulate the knowledge of\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;66;03m# how to turn any given factor into a chunk of data:\u001b[39;00m\n\u001b[1;32m    751\u001b[0m factor_infos \u001b[38;5;241m=\u001b[39m {}\n",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/patsy/build.py:491\u001b[0m, in \u001b[0;36m_examine_factor_types\u001b[0;34m(factors, factor_states, data_iter_maker, NA_action)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m data_iter_maker():\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m factor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(examine_needed):\n\u001b[0;32m--> 491\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mfactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfactor_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m factor \u001b[38;5;129;01min\u001b[39;00m cat_sniffers \u001b[38;5;129;01mor\u001b[39;00m guess_categorical(value):\n\u001b[1;32m    493\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m factor \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m cat_sniffers:\n",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/patsy/eval.py:599\u001b[0m, in \u001b[0;36mEvalFactor.eval\u001b[0;34m(self, memorize_state, data)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21meval\u001b[39m(\u001b[38;5;28mself\u001b[39m, memorize_state, data):\n\u001b[0;32m--> 599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemorize_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_code\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemorize_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/patsy/eval.py:582\u001b[0m, in \u001b[0;36mEvalFactor._eval\u001b[0;34m(self, code, memorize_state, data)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_eval\u001b[39m(\u001b[38;5;28mself\u001b[39m, code, memorize_state, data):\n\u001b[1;32m    581\u001b[0m     inner_namespace \u001b[38;5;241m=\u001b[39m VarLookupDict([data, memorize_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransforms\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[0;32m--> 582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_and_wrap_exc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mError evaluating factor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemorize_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_env\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minner_namespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_namespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/DataAnalytics/.venv/lib/python3.9/site-packages/patsy/compat.py:43\u001b[0m, in \u001b[0;36mcall_and_wrap_exc\u001b[0;34m(msg, origin, f, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     42\u001b[0m     new_exc \u001b[38;5;241m=\u001b[39m PatsyError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (msg, e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, e), origin)\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
            "\u001b[0;31mPatsyError\u001b[0m: Error evaluating factor: NameError: name 'trial_period' is not defined\n    treatment ~ x2 + followup_time + I(followup_time**2) + trial_period + I(trial_period**2)\n                                                           ^^^^^^^^^^^^"
          ]
        }
      ],
      "source": [
        "'''\n",
        "This example demonstrates how to fit a Marginal Structural Model (MSM) using zEpid's IPTW class:\n",
        "\n",
        "1. Load the dataset. Replace load_sample_data(timevary=False) with your actual dataset.\n",
        "2. Specify the exposure (treatment assignment) model using iptw.exposure_model().\n",
        "3. Fit the exposure model using iptw.fit().\n",
        "4. Specify the outcome model (MSM) using iptw.marginal_structural_model().\n",
        "5. Fit the outcome model (MSM) using iptw.fit() again.\n",
        "6. Access the results and summary.\n",
        "'''\n",
        "\n",
        "# Rename the 'dead' column to 'outcome' and 'art' to 'treatment'\n",
        "df = df.rename(columns={'dead': 'outcome', 'art': 'treatment'})  # Assuming 'dead' is the outcome variable and 'art' is the treatment variable in the sample data\n",
        "\n",
        "# Specify the exposure (treatment assignment) model\n",
        "iptw = IPTW(df, treatment='treatment', outcome='outcome')\n",
        "iptw.treatment_model('x2 + followup_time + I(followup_time**2) + trial_period + I(trial_period**2)')\n",
        "\n",
        "# Specify the outcome model (MSM)\n",
        "iptw.marginal_structural_model('assigned_treatment')\n",
        "iptw.fit()\n",
        "\n",
        "# Access results and summary\n",
        "iptw.summary()\n",
        "\n",
        "\n",
        "\n",
        "# Add a constant term for the intercept\n",
        "df['intercept'] = 1.0\n",
        "\n",
        "# Define the independent variables (including the intercept)\n",
        "independent_vars = ['intercept', 'assigned_treatment', 'x2', 'followup_time', 'trial_period']\n",
        "\n",
        "# Fit the weighted logistic regression model\n",
        "model = sm.GLM(df['outcome'], df[independent_vars], family=sm.families.Binomial(), freq_weights=iptw.weights)\n",
        "result = model.fit()\n",
        "\n",
        "# Print the summary of the model\n",
        "print(result.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18eOig9RJL8N",
        "outputId": "a6a6bc83-4b10-49b2-ad16-87c3ae69a7c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['id', 'male', 'age0', 'cd40', 'dvl0', 'treatment', 'outcome', 't',\n",
            "       'cd4_wk45'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(df.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjYbqnMTKqax"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
