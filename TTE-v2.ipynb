{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV9NdqOb69Ql"
      },
      "source": [
        "# SetUp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l_FkFMB_EhyM"
      },
      "outputs": [],
      "source": [
        "#Import Libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import numpy as np\n",
        "from sklearn.utils import resample\n",
        "import zepid\n",
        "from zepid import load_sample_data\n",
        "from zepid.causal.ipw import IPTW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COoqPtol69Qo",
        "outputId": "9551c6bb-392c-4412-ff8c-d672d8096f4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "##################################################\n",
            "## Trial Sequence Object\n",
            "## Estimand: ITT\n",
            "\n",
            "## Data:\n",
            " - N: 725 observations from 89 patients\n",
            "\n",
            "Data Preview:\n",
            "   id  period  treatment  x1        x2  x3        x4  age     age_s  outcome  censored  eligible\n",
            "0   1       0          1   1  1.146148   0  0.734203   36  0.083333        0         0         1\n",
            "1   1       1          1   1  0.002200   0  0.734203   37  0.166667        0         0         0\n",
            "---\n",
            "     id  period  treatment  x1        x2  x3        x4  age     age_s  outcome  censored  eligible\n",
            "723  99       6          1   1 -0.033762   1  0.575268   71  3.000000        0         0         0\n",
            "724  99       7          0   0 -1.340497   1  0.575268   72  3.083333        1         0         0\n",
            "\n",
            "Variable Types:\n",
            "id             int64\n",
            "period         int64\n",
            "treatment      int64\n",
            "x1             int64\n",
            "x2           float64\n",
            "x3             int64\n",
            "x4           float64\n",
            "age            int64\n",
            "age_s        float64\n",
            "outcome        int64\n",
            "censored       int64\n",
            "eligible       int64\n",
            "\n",
            "## IPW for informative censoring:\n",
            " - No weight model specified\n",
            "\n",
            "## Sequence of Trials Data:\n",
            " - Use set_expansion_options() and expand_trials() to construct the sequence of trials dataset.\n",
            "\n",
            "## Outcome model:\n",
            " - Not specified\n",
            "##################################################\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def trial_sequence(estimand):\n",
        "    \"\"\"Initialize a trial sequence with specified estimand.\"\"\"\n",
        "    return {\n",
        "        \"estimand\": estimand,\n",
        "        \"data\": None,\n",
        "        \"ipw_model\": None,\n",
        "        \"outcome_model\": None\n",
        "    }\n",
        "\n",
        "def set_data(trial, data, id_col, period_col, treatment_col, outcome_col, eligible_col):\n",
        "    \"\"\"Prepare and store trial data with relevant columns.\"\"\"\n",
        "    trial[\"data\"] = data[\n",
        "        [id_col, period_col, treatment_col, \"x1\", \"x2\", \"x3\", \"x4\",\n",
        "         \"age\", \"age_s\", outcome_col, \"censored\", eligible_col]\n",
        "    ].copy()\n",
        "    return trial\n",
        "\n",
        "def summarize_trial(trial):\n",
        "    \"\"\"Print a structured summary of the trial object.\"\"\"\n",
        "    print(f\"## Trial Sequence Object\")\n",
        "    print(f\"## Estimand: {trial['estimand']}\\n\")\n",
        "\n",
        "    print(\"## Data:\")\n",
        "    data = trial[\"data\"]\n",
        "    if data is not None:\n",
        "        n_obs = len(data)\n",
        "        n_patients = data[\"id\"].nunique()\n",
        "        print(f\" - N: {n_obs} observations from {n_patients} patients\")\n",
        "\n",
        "        # Data preview with head and tail\n",
        "        print(\"\\nData Preview:\")\n",
        "        with pd.option_context('display.max_columns', None, 'display.expand_frame_repr', False):\n",
        "            print(data.head(2).to_string())\n",
        "            print(\"---\")\n",
        "            print(data.tail(2).to_string())\n",
        "\n",
        "        # Variable types\n",
        "        print(\"\\nVariable Types:\")\n",
        "        print(data.dtypes.to_string())\n",
        "    else:\n",
        "        print(\" - No data loaded\")\n",
        "\n",
        "    # IPW information\n",
        "    print(\"\\n## IPW for informative censoring:\")\n",
        "    print(\" - No weight model specified\" if trial[\"ipw_model\"] is None\n",
        "          else f\" - Model: {trial['ipw_model']}\")\n",
        "\n",
        "    # Trial sequence instructions\n",
        "    print(\"\\n## Sequence of Trials Data:\")\n",
        "    print(\" - Use set_expansion_options() and expand_trials() to construct the sequence of trials dataset.\")\n",
        "\n",
        "    # Outcome model information\n",
        "    print(\"\\n## Outcome model:\")\n",
        "    print(\" - Not specified\" if trial[\"outcome_model\"] is None\n",
        "          else f\" - Model: {trial['outcome_model']}\")\n",
        "\n",
        "# Initialize target trials\n",
        "trial_pp = trial_sequence(estimand=\"PP\")\n",
        "trial_itt = trial_sequence(estimand=\"ITT\")\n",
        "\n",
        "# Create output directories\n",
        "for dir_path in [os.path.join(os.getcwd(), \"trial_pp\"),\n",
        "                 os.path.join(os.getcwd(), \"trial_itt\")]:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "# Load and prepare data\n",
        "data_censored = pd.read_csv(\"data_censored.csv\")\n",
        "\n",
        "# Set up trials\n",
        "trial_pp = set_data(trial_pp, data_censored, \"id\", \"period\", \"treatment\", \"outcome\", \"eligible\")\n",
        "trial_itt = set_data(trial_itt, data_censored, \"id\", \"period\", \"treatment\", \"outcome\", \"eligible\")\n",
        "\n",
        "# Display ITT trial summary\n",
        "print(\"\\n\" + \"#\"*50)\n",
        "summarize_trial(trial_itt)\n",
        "print(\"#\"*50 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVA95vgm-m0B"
      },
      "source": [
        "# 3.1 Censoring due to treatment switching\n",
        "\n",
        "In the R code, the ```set_switch_weight_model()``` function is used to handle censoring due to treatment switching. This involves specifying numerator and denominator models to calculate the probability of receiving treatment in the current period, with separate models for patients based on their previous treatment status.\n",
        "\n",
        "To replicate this in Python, we can use logistic regression models from the ```statsmodels```library to estimate these probabilities. Here's how you can implement this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDaYb90k7BFA",
        "outputId": "444c9f6e-9626-4c0f-e96f-01f9ce7f4142"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                           Logit Regression Results                           \n",
            "==============================================================================\n",
            "Dep. Variable:              treatment   No. Observations:                  725\n",
            "Model:                          Logit   Df Residuals:                      723\n",
            "Method:                           MLE   Df Model:                            1\n",
            "Date:                Sun, 02 Mar 2025   Pseudo R-squ.:                 0.04144\n",
            "Time:                        09:26:57   Log-Likelihood:                -480.24\n",
            "converged:                       True   LL-Null:                       -501.01\n",
            "Covariance Type:            nonrobust   LLR p-value:                 1.163e-10\n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept      1.8867      0.333      5.671      0.000       1.235       2.539\n",
            "age           -0.0421      0.007     -6.213      0.000      -0.055      -0.029\n",
            "==============================================================================\n",
            "                           Logit Regression Results                           \n",
            "==============================================================================\n",
            "Dep. Variable:              treatment   No. Observations:                  725\n",
            "Model:                          Logit   Df Residuals:                      721\n",
            "Method:                           MLE   Df Model:                            3\n",
            "Date:                Sun, 02 Mar 2025   Pseudo R-squ.:                 0.04459\n",
            "Time:                        09:26:57   Log-Likelihood:                -478.67\n",
            "converged:                       True   LL-Null:                       -501.01\n",
            "Covariance Type:            nonrobust   LLR p-value:                 1.084e-09\n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept      1.8308      0.356      5.145      0.000       1.133       2.528\n",
            "age           -0.0429      0.007     -6.261      0.000      -0.056      -0.029\n",
            "x1             0.2744      0.157      1.752      0.080      -0.033       0.581\n",
            "x3            -0.0321      0.155     -0.207      0.836      -0.336       0.272\n",
            "==============================================================================\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Step 1: We define a helper function fit_logistic_regression to fit logistic regression models using statsmodels.\n",
        "Step 2: The set_switch_weight_model function fits the numerator and denominator models based on the provided formulas and stores them in the trial object.\n",
        "Step 3: We apply this function to the per-protocol trial (trial_pp) with the specified formulas.\n",
        "'''\n",
        "\n",
        "\n",
        "#Step 1\n",
        "def fit_logistic_regression(formula, data):\n",
        "    \"\"\"Fit a logistic regression model.\"\"\"\n",
        "    model = sm.Logit.from_formula(formula, data)\n",
        "    result = model.fit(disp=False)\n",
        "    return result\n",
        "\n",
        "#Step 2\n",
        "def set_switch_weight_model(trial, numerator_formula, denominator_formula, save_path=None):\n",
        "    \"\"\"\n",
        "    Set up the switch weight models for treatment switching.\n",
        "    \"\"\"\n",
        "    data = trial[\"data\"].copy()\n",
        "\n",
        "    # Fit numerator model\n",
        "    numerator_model = fit_logistic_regression(numerator_formula, data)\n",
        "\n",
        "    # Fit denominator model\n",
        "    denominator_model = fit_logistic_regression(denominator_formula, data)\n",
        "\n",
        "    # Store models in the trial object\n",
        "    trial[\"switch_weights\"] = {\n",
        "        \"numerator_model\": numerator_model,\n",
        "        \"denominator_model\": denominator_model\n",
        "    }\n",
        "\n",
        "    # Optionally, save the models\n",
        "    if save_path:\n",
        "        # Create the directory if it doesn't exist\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        numerator_model.save(f\"{save_path}/numerator_model.pickle\")\n",
        "        denominator_model.save(f\"{save_path}/denominator_model.pickle\")\n",
        "\n",
        "    return trial\n",
        "\n",
        "# Define formulas\n",
        "numerator_formula = 'treatment ~ age'\n",
        "denominator_formula = 'treatment ~ age + x1 + x3'\n",
        "\n",
        "# Apply to the per-protocol trial\n",
        "trial_pp = set_switch_weight_model(\n",
        "    trial_pp,\n",
        "    numerator_formula=numerator_formula,\n",
        "    denominator_formula=denominator_formula,\n",
        "    save_path='trial_pp/switch_models'\n",
        ")\n",
        "\n",
        "# Display switch weight models summary\n",
        "print(trial_pp[\"switch_weights\"][\"numerator_model\"].summary())\n",
        "print(trial_pp[\"switch_weights\"][\"denominator_model\"].summary())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHsDwHV8_eAl"
      },
      "source": [
        "# Step 3.2: Other Informative Censoring\n",
        "\n",
        "For other types of informative censoring, the R code uses the ```set_censor_weight_model()``` function to estimate inverse probability of censoring weights (IPCW). In Python, we can similarly fit logistic regression models to estimate the probability of censoring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBrx9ZU9_7Oq",
        "outputId": "b801545f-1261-4335-db1a-33bd68cf509e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                           Logit Regression Results                           \n",
            "==============================================================================\n",
            "Dep. Variable:           not_censored   No. Observations:                  725\n",
            "Model:                          Logit   Df Residuals:                      723\n",
            "Method:                           MLE   Df Model:                            1\n",
            "Date:                Sun, 02 Mar 2025   Pseudo R-squ.:                 0.02676\n",
            "Time:                        09:26:57   Log-Likelihood:                -196.70\n",
            "converged:                       True   LL-Null:                       -202.11\n",
            "Covariance Type:            nonrobust   LLR p-value:                  0.001007\n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept      2.4481      0.141     17.415      0.000       2.173       2.724\n",
            "x2            -0.4486      0.137     -3.278      0.001      -0.717      -0.180\n",
            "==============================================================================\n",
            "                           Logit Regression Results                           \n",
            "==============================================================================\n",
            "Dep. Variable:           not_censored   No. Observations:                  725\n",
            "Model:                          Logit   Df Residuals:                      722\n",
            "Method:                           MLE   Df Model:                            2\n",
            "Date:                Sun, 02 Mar 2025   Pseudo R-squ.:                 0.04069\n",
            "Time:                        09:26:57   Log-Likelihood:                -193.88\n",
            "converged:                       True   LL-Null:                       -202.11\n",
            "Covariance Type:            nonrobust   LLR p-value:                 0.0002679\n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept      2.2059      0.165     13.339      0.000       1.882       2.530\n",
            "x2            -0.4706      0.137     -3.423      0.001      -0.740      -0.201\n",
            "x1             0.7019      0.307      2.285      0.022       0.100       1.304\n",
            "==============================================================================\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Step 1: We add a not_censored column to indicate whether an observation is not censored.\n",
        "Step 2: The set_censor_weight_model function fits numerator and denominator models to estimate the probability of not being censored, based on the provided formulas.\n",
        "Step 3: We apply this function to both the per-protocol (trial_pp) and intention-to-treat (trial_itt) trials.\n",
        "'''\n",
        "\n",
        "def set_censor_weight_model(trial, censor_event_col, numerator_formula, denominator_formula, save_path=None):\n",
        "    \"\"\"\n",
        "    Set up the censor weight models for informative censoring.\n",
        "    \"\"\"\n",
        "    data = trial[\"data\"].copy()\n",
        "\n",
        "    # Create the censoring indicator (1 - censored)\n",
        "    data['not_censored'] = 1 - data[censor_event_col]\n",
        "\n",
        "    # Fit numerator model\n",
        "    numerator_model = fit_logistic_regression(numerator_formula, data)\n",
        "\n",
        "    # Fit denominator model\n",
        "    denominator_model = fit_logistic_regression(denominator_formula, data)\n",
        "\n",
        "    # Store models in the trial object\n",
        "    trial[\"censor_weights\"] = {\n",
        "        \"numerator_model\": numerator_model,\n",
        "        \"denominator_model\": denominator_model\n",
        "    }\n",
        "\n",
        "    # Optionally, save the models\n",
        "    if save_path:\n",
        "        # Ensure the directory exists before saving.\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        numerator_model.save(os.path.join(save_path, \"numerator_model.pickle\")) # Use os.path.join to create the full file path\n",
        "        denominator_model.save(os.path.join(save_path, \"denominator_model.pickle\"))  # Use os.path.join to create the full file path\n",
        "\n",
        "    return trial\n",
        "\n",
        "# Define formulas\n",
        "numerator_formula = 'not_censored ~ x2'\n",
        "denominator_formula = 'not_censored ~ x2 + x1'\n",
        "\n",
        "# Apply to the per-protocol trial\n",
        "trial_pp = set_censor_weight_model(\n",
        "    trial_pp,\n",
        "    censor_event_col='censored',\n",
        "    numerator_formula=numerator_formula,\n",
        "    denominator_formula=denominator_formula,\n",
        "    save_path='trial_pp/censor_models'\n",
        ")\n",
        "\n",
        "# Apply to the intention-to-treat trial\n",
        "trial_itt = set_censor_weight_model(\n",
        "    trial_itt,\n",
        "    censor_event_col='censored',\n",
        "    numerator_formula=numerator_formula,\n",
        "    denominator_formula=denominator_formula,\n",
        "    save_path='trial_itt/censor_models'\n",
        ")\n",
        "\n",
        "# Display censor weight models summary for ITT trial\n",
        "print(trial_itt[\"censor_weights\"][\"numerator_model\"].summary())\n",
        "print(trial_itt[\"censor_weights\"][\"denominator_model\"].summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPw94_onAhpS"
      },
      "source": [
        "# Step 4: Calculate Weights\n",
        "\n",
        "In the R code, the ```calculate_weights()``` function is used to fit the models specified earlier and compute the inverse probability weights. To replicate this functionality in Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ld40cliBUbF",
        "outputId": "10056c11-fa08-4e55-c5ea-392a8a537041"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Treatment Weight Models:\n",
            "\n",
            "Numerator Model Summary:\n",
            "                           Logit Regression Results                           \n",
            "==============================================================================\n",
            "Dep. Variable:              treatment   No. Observations:                  725\n",
            "Model:                          Logit   Df Residuals:                      723\n",
            "Method:                           MLE   Df Model:                            1\n",
            "Date:                Sun, 02 Mar 2025   Pseudo R-squ.:                 0.04144\n",
            "Time:                        09:26:57   Log-Likelihood:                -480.24\n",
            "converged:                       True   LL-Null:                       -501.01\n",
            "Covariance Type:            nonrobust   LLR p-value:                 1.163e-10\n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept      1.8867      0.333      5.671      0.000       1.235       2.539\n",
            "age           -0.0421      0.007     -6.213      0.000      -0.055      -0.029\n",
            "==============================================================================\n",
            "\n",
            "Denominator Model Summary:\n",
            "                           Logit Regression Results                           \n",
            "==============================================================================\n",
            "Dep. Variable:              treatment   No. Observations:                  725\n",
            "Model:                          Logit   Df Residuals:                      721\n",
            "Method:                           MLE   Df Model:                            3\n",
            "Date:                Sun, 02 Mar 2025   Pseudo R-squ.:                 0.04459\n",
            "Time:                        09:26:57   Log-Likelihood:                -478.67\n",
            "converged:                       True   LL-Null:                       -501.01\n",
            "Covariance Type:            nonrobust   LLR p-value:                 1.084e-09\n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept      1.8308      0.356      5.145      0.000       1.133       2.528\n",
            "age           -0.0429      0.007     -6.261      0.000      -0.056      -0.029\n",
            "x1             0.2744      0.157      1.752      0.080      -0.033       0.581\n",
            "x3            -0.0321      0.155     -0.207      0.836      -0.336       0.272\n",
            "==============================================================================\n",
            "\n",
            "Censoring Weight Models:\n",
            "\n",
            "Numerator Model Summary:\n",
            "                           Logit Regression Results                           \n",
            "==============================================================================\n",
            "Dep. Variable:           not_censored   No. Observations:                  725\n",
            "Model:                          Logit   Df Residuals:                      723\n",
            "Method:                           MLE   Df Model:                            1\n",
            "Date:                Sun, 02 Mar 2025   Pseudo R-squ.:                 0.02676\n",
            "Time:                        09:26:57   Log-Likelihood:                -196.70\n",
            "converged:                       True   LL-Null:                       -202.11\n",
            "Covariance Type:            nonrobust   LLR p-value:                  0.001007\n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept      2.4481      0.141     17.415      0.000       2.173       2.724\n",
            "x2            -0.4486      0.137     -3.278      0.001      -0.717      -0.180\n",
            "==============================================================================\n",
            "\n",
            "Denominator Model Summary:\n",
            "                           Logit Regression Results                           \n",
            "==============================================================================\n",
            "Dep. Variable:           not_censored   No. Observations:                  725\n",
            "Model:                          Logit   Df Residuals:                      722\n",
            "Method:                           MLE   Df Model:                            2\n",
            "Date:                Sun, 02 Mar 2025   Pseudo R-squ.:                 0.04069\n",
            "Time:                        09:26:57   Log-Likelihood:                -193.88\n",
            "converged:                       True   LL-Null:                       -202.11\n",
            "Covariance Type:            nonrobust   LLR p-value:                 0.0002679\n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept      2.2059      0.165     13.339      0.000       1.882       2.530\n",
            "x2            -0.4706      0.137     -3.423      0.001      -0.740      -0.201\n",
            "x1             0.7019      0.307      2.285      0.022       0.100       1.304\n",
            "==============================================================================\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Step 1: Fit the Numerator and Denominator Models: Utilize the logistic regression models defined previously to estimate the probabilities of treatment and censoring.\n",
        "\n",
        "Step 2: Compute the Weights: Calculate the stabilized weights based on the fitted models.\n",
        "\n",
        "Step 3: Store and Summarize the Models: Save the model summaries for inspection and validation.\n",
        "'''\n",
        "\n",
        "def calculate_weights(trial, save_path=None):\n",
        "    \"\"\"\n",
        "    Fit the weight models and calculate the inverse probability weights.\n",
        "    \"\"\"\n",
        "    data = trial[\"data\"].copy()\n",
        "\n",
        "    # Ensure necessary columns are present\n",
        "    required_cols = ['treatment', 'censored', 'age', 'x1', 'x2', 'x3']\n",
        "    for col in required_cols:\n",
        "        if col not in data.columns:\n",
        "            raise ValueError(f\"Column '{col}' is missing from the data.\")\n",
        "\n",
        "    # Fit treatment models\n",
        "    # Numerator model: P(treatment | age)\n",
        "    numerator_model = sm.Logit.from_formula('treatment ~ age', data).fit(disp=False)\n",
        "\n",
        "    # Denominator model: P(treatment | age + x1 + x3)\n",
        "    denominator_model = sm.Logit.from_formula('treatment ~ age + x1 + x3', data).fit(disp=False)\n",
        "\n",
        "    # Predict probabilities\n",
        "    data['p_treatment_num'] = numerator_model.predict(data)\n",
        "    data['p_treatment_denom'] = denominator_model.predict(data)\n",
        "\n",
        "    # Calculate treatment weights\n",
        "    data['treatment_weight'] = data['p_treatment_num'] / data['p_treatment_denom']\n",
        "\n",
        "    # Fit censoring models\n",
        "    # Numerator model: P(not_censored | x2)\n",
        "    data['not_censored'] = 1 - data['censored']\n",
        "    censor_numerator_model = sm.Logit.from_formula('not_censored ~ x2', data).fit(disp=False)\n",
        "\n",
        "    # Denominator model: P(not_censored | x2 + x1)\n",
        "    censor_denominator_model = sm.Logit.from_formula('not_censored ~ x2 + x1', data).fit(disp=False)\n",
        "\n",
        "    # Predict probabilities\n",
        "    data['p_not_censored_num'] = censor_numerator_model.predict(data)\n",
        "    data['p_not_censored_denom'] = censor_denominator_model.predict(data)\n",
        "\n",
        "    # Calculate censoring weights\n",
        "    data['censoring_weight'] = data['p_not_censored_num'] / data['p_not_censored_denom']\n",
        "\n",
        "    # Combine weights\n",
        "    data['ip_weight'] = data['treatment_weight'] * data['censoring_weight']\n",
        "\n",
        "    # Store the updated data and models in the trial object\n",
        "    trial[\"data\"] = data\n",
        "    trial[\"weight_models\"] = {\n",
        "        \"treatment\": {\n",
        "            \"numerator\": numerator_model,\n",
        "            \"denominator\": denominator_model\n",
        "        },\n",
        "        \"censoring\": {\n",
        "            \"numerator\": censor_numerator_model,\n",
        "            \"denominator\": censor_denominator_model\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Optionally, save the models\n",
        "    if save_path:\n",
        "        # Create the directory if it doesn't exist\n",
        "        os.makedirs(save_path, exist_ok=True)  # This line ensures the directory exists\n",
        "        numerator_model.save(os.path.join(save_path, \"treatment_numerator_model.pickle\")) # Use os.path.join to create the full file path\n",
        "        denominator_model.save(os.path.join(save_path, \"treatment_denominator_model.pickle\"))  # Use os.path.join to create the full file path\n",
        "        censor_numerator_model.save(os.path.join(save_path, \"censor_numerator_model.pickle\")) # Use os.path.join to create the full file path\n",
        "        censor_denominator_model.save(os.path.join(save_path, \"censor_denominator_model.pickle\"))  # Use os.path.join to create the full file path\n",
        "\n",
        "    return trial\n",
        "\n",
        "\n",
        "def show_weight_models(trial):\n",
        "    \"\"\"\n",
        "    Display summaries of the weight models.\n",
        "    \"\"\"\n",
        "    weight_models = trial.get(\"weight_models\", {})\n",
        "\n",
        "    if \"treatment\" in weight_models:\n",
        "        print(\"\\nTreatment Weight Models:\")\n",
        "        print(\"\\nNumerator Model Summary:\")\n",
        "        print(weight_models[\"treatment\"][\"numerator\"].summary())\n",
        "        print(\"\\nDenominator Model Summary:\")\n",
        "        print(weight_models[\"treatment\"][\"denominator\"].summary())\n",
        "\n",
        "    if \"censoring\" in weight_models:\n",
        "        print(\"\\nCensoring Weight Models:\")\n",
        "        print(\"\\nNumerator Model Summary:\")\n",
        "        print(weight_models[\"censoring\"][\"numerator\"].summary())\n",
        "        print(\"\\nDenominator Model Summary:\")\n",
        "        print(weight_models[\"censoring\"][\"denominator\"].summary())\n",
        "\n",
        "# Calculate weights for the per-protocol trial\n",
        "trial_pp = calculate_weights(trial_pp, save_path='trial_pp/weight_models')\n",
        "\n",
        "# Calculate weights for the intention-to-treat trial\n",
        "trial_itt = calculate_weights(trial_itt, save_path='trial_itt/weight_models')\n",
        "\n",
        "# Display weight model summaries for the ITT trial\n",
        "show_weight_models(trial_itt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuHgQOVzCfgU"
      },
      "source": [
        "# Step 5: Set Expansion Options\n",
        "\n",
        "In this step, we define how the trial data should be expanded by specifying parameters such as the output method and chunk size. This setup prepares the trial object for the expansion process.\n",
        "\n",
        "## ```set_expansion_options``` Function:\n",
        "Adds an ```expansion_options``` dictionary to the trial object, containing the output function and chunk size.\n",
        "\n",
        "## ```save_to_csv``` Function:\n",
        "Generates a function that saves each data chunk to a CSV file in the specified directory. This function is assigned to the ```output_func``` parameter in ```set_expansion_options```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PK2Rc14DDcG"
      },
      "outputs": [],
      "source": [
        "def set_expansion_options(trial, output_func, chunk_size=500):\n",
        "    \"\"\"\n",
        "    Set options for expanding the trial data.\n",
        "\n",
        "    Parameters:\n",
        "    - trial (dict): The trial object containing data and configurations.\n",
        "    - output_func (function): Function to process and save each data chunk.\n",
        "    - chunk_size (int): Number of patients to include in each expansion iteration.\n",
        "\n",
        "    Returns:\n",
        "    - dict: Updated trial object with expansion options set.\n",
        "    \"\"\"\n",
        "    trial[\"expansion_options\"] = {\n",
        "        \"output_func\": output_func,\n",
        "        \"chunk_size\": chunk_size\n",
        "    }\n",
        "    return trial\n",
        "\n",
        "def save_to_csv(directory):\n",
        "    \"\"\"\n",
        "    Creates a function to save DataFrame chunks to CSV files in the specified directory.\n",
        "\n",
        "    Parameters:\n",
        "    - directory (str): Directory path where CSV files will be saved.\n",
        "\n",
        "    Returns:\n",
        "    - function: A function that takes a DataFrame chunk and saves it as a CSV file.\n",
        "    \"\"\"\n",
        "    def process_chunk(chunk, chunk_id):\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "        file_path = os.path.join(directory, f\"expanded_data_chunk_{chunk_id}.csv\")\n",
        "        chunk.to_csv(file_path, index=False)\n",
        "        print(f\"Saved: {file_path}\")\n",
        "    return process_chunk\n",
        "\n",
        "# Set expansion options for the per-protocol trial\n",
        "trial_pp = set_expansion_options(\n",
        "    trial_pp,\n",
        "    output_func=save_to_csv(\"trial_pp/expanded_data\"),\n",
        "    chunk_size=500\n",
        ")\n",
        "\n",
        "# Set expansion options for the intention-to-treat trial\n",
        "trial_itt = set_expansion_options(\n",
        "    trial_itt,\n",
        "    output_func=save_to_csv(\"trial_itt/expanded_data\"),\n",
        "    chunk_size=500\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b55fBKHyCjBW"
      },
      "source": [
        "# Step 6: Expand Trials\n",
        "\n",
        "With the expansion options set, this step involves processing the trial data in chunks as specified, applying the output function to each chunk to generate the expanded dataset.\n",
        "\n",
        "##```expand_trials``` Function:\n",
        "Retrieves the data and expansion options from the trial object, divides the data into chunks based on the specified chunk size, and applies the output function to each chunk. The ```output_func``` is called with the data chunk and its corresponding chunk index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hl_3-BpA2qP"
      },
      "outputs": [],
      "source": [
        "def expand_trials(trial, censor_at_switch=True, first_period=0, last_period=10):\n",
        "    \"\"\"\n",
        "    Expand the trial data to emulate a sequence of target trials.\n",
        "\n",
        "    Parameters:\n",
        "    - trial (dict): The trial object containing the original data.\n",
        "    - censor_at_switch (bool): Whether to censor data at treatment switch.\n",
        "    - first_period (int): The starting period for the trials.\n",
        "    - last_period (int or float): The ending period for the trials.\n",
        "\n",
        "    Returns:\n",
        "    - dict: Updated trial object with expanded data.\n",
        "    \"\"\"\n",
        "    data = trial.get(\"data\")\n",
        "    if data is None:\n",
        "        raise ValueError(\"No data found in the trial object.\")\n",
        "\n",
        "    expanded_rows = []\n",
        "\n",
        "    for _, patient_data in data.groupby('id'):\n",
        "        patient_data = patient_data.sort_values(by='period')\n",
        "        start_age = patient_data['age'].iloc[0]\n",
        "        assigned_treatment = patient_data['treatment'].iloc[0]\n",
        "\n",
        "        for trial_period in range(first_period, int(last_period) + 1):\n",
        "            followup_time = 0\n",
        "            for _, row in patient_data.iterrows():\n",
        "                if censor_at_switch and row['treatment'] != assigned_treatment:\n",
        "                    break\n",
        "\n",
        "                expanded_row = row.copy()\n",
        "                expanded_row['trial_period'] = trial_period\n",
        "                expanded_row['followup_time'] = followup_time\n",
        "                expanded_row['assigned_treatment'] = assigned_treatment\n",
        "                expanded_rows.append(expanded_row)\n",
        "\n",
        "                followup_time += 1\n",
        "\n",
        "    expanded_data = pd.DataFrame(expanded_rows)\n",
        "    trial['expanded_data'] = expanded_data\n",
        "\n",
        "    return trial\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FncaIAlPEN99"
      },
      "source": [
        "# Step 6.1: Create Sequence of Trials Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJFG4Ef4EC0O",
        "outputId": "7e28a7a4-5056-453f-e785-7d2193c9e475"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    id  period  treatment   x1        x2   x3        x4   age     age_s  \\\n",
            "0  1.0     0.0        1.0  1.0  1.146148  0.0  0.734203  36.0  0.083333   \n",
            "1  1.0     1.0        1.0  1.0  0.002200  0.0  0.734203  37.0  0.166667   \n",
            "2  1.0     2.0        1.0  0.0 -0.481762  0.0  0.734203  38.0  0.250000   \n",
            "3  1.0     3.0        1.0  0.0  0.007872  0.0  0.734203  39.0  0.333333   \n",
            "4  1.0     4.0        1.0  1.0  0.216054  0.0  0.734203  40.0  0.416667   \n",
            "\n",
            "   outcome  ...  p_treatment_denom  treatment_weight  not_censored  \\\n",
            "0      0.0  ...           0.636513          0.930088           1.0   \n",
            "1      0.0  ...           0.626528          0.928634           1.0   \n",
            "2      0.0  ...           0.549849          1.039459           1.0   \n",
            "3      0.0  ...           0.539206          1.040816           1.0   \n",
            "4      0.0  ...           0.595948          0.924292           1.0   \n",
            "\n",
            "   p_not_censored_num  p_not_censored_denom  censoring_weight  ip_weight  \\\n",
            "0            0.873678              0.914385          0.955481   0.888682   \n",
            "1            0.920349              0.948181          0.970647   0.901376   \n",
            "2            0.934883              0.919281          1.016972   1.057101   \n",
            "3            0.920163              0.900444          1.021899   1.063608   \n",
            "4            0.913026              0.943007          0.968208   0.894907   \n",
            "\n",
            "   trial_period  followup_time  assigned_treatment  \n",
            "0           0.0            0.0                 1.0  \n",
            "1           0.0            1.0                 1.0  \n",
            "2           0.0            2.0                 1.0  \n",
            "3           0.0            3.0                 1.0  \n",
            "4           0.0            4.0                 1.0  \n",
            "\n",
            "[5 rows x 23 columns]\n"
          ]
        }
      ],
      "source": [
        "# Example usage:\n",
        "trial_pp = expand_trials(trial_pp)\n",
        "trial_itt = expand_trials(trial_itt)\n",
        "\n",
        "# Display the first few rows of the expanded data for the per-protocol trial\n",
        "print(trial_pp['expanded_data'].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPhhWmb3FWCL"
      },
      "source": [
        "# Step 7: Load or Sample from Expanded Data\n",
        "\n",
        "After expanding the trial data, we prepare it for fitting the outcome model. If the dataset is manageable in size, we can load it directly into memory. For larger datasets, sampling may be necessary to ensure efficient processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8rAcgOBGVlF"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "In this snippet:\n",
        "\n",
        "  We set a random seed for reproducibility.\n",
        "  We define the proportion (p_control) of control observations (where outcome == 0) to include in the sampled data.\n",
        "  We separate the treated and control observations.\n",
        "  We sample from the control observations based on the defined proportion.\n",
        "  We combine the treated observations with the sampled control observations to create the sampled_data DataFrame.\n",
        "'''\n",
        "\n",
        "# Assuming 'expanded_data' is your expanded DataFrame\n",
        "# For large datasets, sample a subset; for small datasets, you might skip this step\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "random_seed = 1234\n",
        "\n",
        "# Define the proportion of control observations to include\n",
        "p_control = 0.5\n",
        "\n",
        "# Access the 'expanded_data' from the trial dictionary\n",
        "expanded_data = trial_pp['expanded_data']  # or trial_itt['expanded_data'], depending on which trial you want to sample from\n",
        "\n",
        "# Separate treated and control observations\n",
        "treated = expanded_data[expanded_data['outcome'] == 1]\n",
        "control = expanded_data[expanded_data['outcome'] == 0]\n",
        "\n",
        "# Sample from control observations\n",
        "control_sampled = resample(control, replace=False, n_samples=int(p_control * len(control)), random_state=random_seed)\n",
        "\n",
        "# Combine treated observations with sampled control observations\n",
        "sampled_data = pd.concat([treated, control_sampled])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD2YVfB5Gmr4"
      },
      "source": [
        "# Step 8: Fit Marginal Structural Model\n",
        "\n",
        "To fit an MSM, we utilize inverse probability weighting (IPW). The ```zEpid``` library provides tools to calculate these weights and fit the MSM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "Hkl4zdd8GJYe",
        "outputId": "30c8c192-0253-44c6-83e8-110a88681995"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/zepid/datasets/__init__.py:60: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  df = pd.read_csv(resource_filename('zepid', 'datasets/data.dat'),\n",
            "/usr/local/lib/python3.11/dist-packages/zepid/datasets/__init__.py:60: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
            "  df = pd.read_csv(resource_filename('zepid', 'datasets/data.dat'),\n",
            "/usr/local/lib/python3.11/dist-packages/zepid/causal/utils.py:58: UserWarning: There is missing data that is not the outcome in the data set. IPTW will drop all missing data that is not missing outcome data. IPTW will fit 460 of 547 observations\n",
            "  warnings.warn(\"There is missing data that is not the outcome in the data set. \" + str(estimator) +\n"
          ]
        },
        {
          "ename": "PatsyError",
          "evalue": "Error evaluating factor: NameError: name 'followup_time' is not defined\n    treatment ~ x2 + followup_time + I(followup_time**2) + trial_period + I(trial_period**2)\n                                     ^^^^^^^^^^^^^^^^^^^",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/patsy/compat.py\u001b[0m in \u001b[0;36mcall_and_wrap_exc\u001b[0;34m(msg, origin, f, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/patsy/eval.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, expr, source_name, inner_namespace)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVarLookupDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minner_namespace\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_namespaces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'followup_time' is not defined",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mPatsyError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-527074fb83c0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Specify the exposure (treatment assignment) model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0miptw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIPTW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreatment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'treatment'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutcome\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'outcome'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0miptw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtreatment_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x2 + followup_time + I(followup_time**2) + trial_period + I(trial_period**2)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Specify the outcome model (MSM)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/zepid/causal/ipw/IPTW.py\u001b[0m in \u001b[0;36mtreatment_model\u001b[0;34m(self, model_denominator, model_numerator, stabilized, bound, print_results)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;31m# Calculating denominator probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_denominator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         self.df['__denom__'], self.df['__numer__'], self.iptw = iptw_calculator(df=self.df,\n\u001b[0m\u001b[1;32m    223\u001b[0m                                                                                 \u001b[0mtreatment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtreatment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                                                                                 \u001b[0mmodel_denom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_denominator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/zepid/causal/utils.py\u001b[0m in \u001b[0;36miptw_calculator\u001b[0;34m(df, treatment, model_denom, model_numer, weight, stabilized, standardize, bound, print_results)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0mAIPSW\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \"\"\"\n\u001b[0;32m--> 304\u001b[0;31m     denominator_model = propensity_score(df, treatment + ' ~ ' + model_denom,\n\u001b[0m\u001b[1;32m    305\u001b[0m                                          weights=weight, print_results=print_results)\n\u001b[1;32m    306\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdenominator_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/zepid/causal/utils.py\u001b[0m in \u001b[0;36mpropensity_score\u001b[0;34m(df, model, weights, print_results)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfamilies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfamily\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36mfrom_formula\u001b[0;34m(cls, formula, data, subset, drop_cols, *args, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'raise'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         tmp = handle_formula_data(data, None, formula, depth=eval_env,\n\u001b[0m\u001b[1;32m    204\u001b[0m                                   missing=missing)\n\u001b[1;32m    205\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesign_info\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/formula/formulatools.py\u001b[0m in \u001b[0;36mhandle_formula_data\u001b[0;34m(Y, X, formula, depth, missing)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_using_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             result = dmatrices(formula, Y, depth, return_type='dataframe',\n\u001b[0m\u001b[1;32m     64\u001b[0m                                NA_action=na_action)\n\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/patsy/highlevel.py\u001b[0m in \u001b[0;36mdmatrices\u001b[0;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \"\"\"\n\u001b[1;32m    318\u001b[0m     \u001b[0meval_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvalEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     (lhs, rhs) = _do_highlevel_design(\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0mformula_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNA_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/patsy/highlevel.py\u001b[0m in \u001b[0;36m_do_highlevel_design\u001b[0;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     design_infos = _try_incr_builders(\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mformula_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_iter_maker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNA_action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/patsy/highlevel.py\u001b[0m in \u001b[0;36m_try_incr_builders\u001b[0;34m(formula_like, data_iter_maker, eval_env, NA_action)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformula_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelDesc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEvalEnvironment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         return design_matrix_builders(\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mformula_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlhs_termlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformula_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrhs_termlist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mdata_iter_maker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/patsy/build.py\u001b[0m in \u001b[0;36mdesign_matrix_builders\u001b[0;34m(termlists, data_iter_maker, eval_env, NA_action)\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0;31m# Now all the factors have working eval methods, so we can evaluate them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m     \u001b[0;31m# on some data to find out what type of data they return.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m     (num_column_counts, cat_levels_contrasts) = _examine_factor_types(\n\u001b[0m\u001b[1;32m    747\u001b[0m         \u001b[0mall_factors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_iter_maker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNA_action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/patsy/build.py\u001b[0m in \u001b[0;36m_examine_factor_types\u001b[0;34m(factors, factor_states, data_iter_maker, NA_action)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_iter_maker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamine_needed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactor_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfactor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcat_sniffers\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mguess_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcat_sniffers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/patsy/eval.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, memorize_state, data)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemorize_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemorize_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_code\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemorize_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0m__getstate__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mno_pickling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/patsy/eval.py\u001b[0m in \u001b[0;36m_eval\u001b[0;34m(self, code, memorize_state, data)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemorize_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0minner_namespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVarLookupDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemorize_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"transforms\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m         return call_and_wrap_exc(\n\u001b[0m\u001b[1;32m    583\u001b[0m             \u001b[0;34m\"Error evaluating factor\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/patsy/compat.py\u001b[0m in \u001b[0;36mcall_and_wrap_exc\u001b[0;34m(msg, origin, f, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mnew_exc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPatsyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mnew_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mPatsyError\u001b[0m: Error evaluating factor: NameError: name 'followup_time' is not defined\n    treatment ~ x2 + followup_time + I(followup_time**2) + trial_period + I(trial_period**2)\n                                     ^^^^^^^^^^^^^^^^^^^"
          ]
        }
      ],
      "source": [
        "'''\n",
        "This example demonstrates how to fit a Marginal Structural Model (MSM) using zEpid's IPTW class:\n",
        "\n",
        "1. Load the dataset. Replace load_sample_data(timevary=False) with your actual dataset.\n",
        "2. Specify the exposure (treatment assignment) model using iptw.exposure_model().\n",
        "3. Fit the exposure model using iptw.fit().\n",
        "4. Specify the outcome model (MSM) using iptw.marginal_structural_model().\n",
        "5. Fit the outcome model (MSM) using iptw.fit() again.\n",
        "6. Access the results and summary.\n",
        "'''\n",
        "\n",
        "# Load your data\n",
        "# For demonstration, we'll use zEpid's sample data\n",
        "df = load_sample_data(timevary=False)\n",
        "\n",
        "# Rename the 'dead' column to 'outcome' and 'art' to 'treatment'\n",
        "df = df.rename(columns={'dead': 'outcome', 'art': 'treatment'})  # Assuming 'dead' is the outcome variable and 'art' is the treatment variable in the sample data\n",
        "\n",
        "# Specify the exposure (treatment assignment) model\n",
        "iptw = IPTW(df, treatment='treatment', outcome='outcome')\n",
        "iptw.treatment_model('x2 + followup_time + I(followup_time**2) + trial_period + I(trial_period**2)')\n",
        "\n",
        "# Specify the outcome model (MSM)\n",
        "iptw.marginal_structural_model('assigned_treatment')\n",
        "iptw.fit()\n",
        "\n",
        "# Access results and summary\n",
        "iptw.summary()\n",
        "\n",
        "\n",
        "\n",
        "# Add a constant term for the intercept\n",
        "df['intercept'] = 1.0\n",
        "\n",
        "# Define the independent variables (including the intercept)\n",
        "independent_vars = ['intercept', 'assigned_treatment', 'x2', 'followup_time', 'trial_period']\n",
        "\n",
        "# Fit the weighted logistic regression model\n",
        "model = sm.GLM(df['outcome'], df[independent_vars], family=sm.families.Binomial(), freq_weights=iptw.weights)\n",
        "result = model.fit()\n",
        "\n",
        "# Print the summary of the model\n",
        "print(result.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18eOig9RJL8N",
        "outputId": "a6a6bc83-4b10-49b2-ad16-87c3ae69a7c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['id', 'male', 'age0', 'cd40', 'dvl0', 'treatment', 'outcome', 't',\n",
            "       'cd4_wk45'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(df.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjYbqnMTKqax"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
